{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, mem_size, batch_size, input_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_centr = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.state_memory = np.zeros(\n",
    "            (self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros(\n",
    "            (self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "\n",
    "    def store_transitions(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_centr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "\n",
    "        self.mem_centr = self.mem_centr + 1\n",
    "\n",
    "    def is_sampleable(self):\n",
    "        if self.mem_centr >= self.batch_size:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        if not(self.is_sampleable()):\n",
    "            return []\n",
    "        \n",
    "        max_mem = min(self.mem_size, self.mem_centr)\n",
    "        \n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminals = self.terminal_memory[batch]\n",
    "\n",
    "        return states, new_states, actions, rewards, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(lr, n_actions):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(100, activation='relu'),\n",
    "        keras.layers.Dense(100, activation='relu'),\n",
    "        keras.layers.Dense(n_actions, activation=None)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, input_dims,\n",
    "                 lr=1e-4, gamma=0.9, mem_size=128, batch_size=64,\n",
    "                  epsilon_decay=0.995, target_update_frequency=256):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_frequency\n",
    "\n",
    "        self.policy_network = build_dqn(lr=lr, n_actions=n_actions)\n",
    "        self.target_network = deepcopy(self.policy_network)\n",
    "\n",
    "        self.replay_mem = ReplayBuffer(\n",
    "            mem_size=mem_size, batch_size=batch_size, input_dims=input_dims)\n",
    "\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            obs = np.array([obs])\n",
    "            policy_values = self.policy_network.predict(obs)\n",
    "            action = np.argmax(policy_values)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_memory(self, state, action, reward, new_state, done):\n",
    "        self.replay_mem.store_transitions(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if not(self.replay_mem.is_sampleable()):\n",
    "            return 0\n",
    "\n",
    "        states, new_states, actions, rewards, dones = self.replay_mem.sample_buffer()\n",
    "\n",
    "        q_eval = self.policy_network.predict(states)\n",
    "        q_next = self.target_network.predict(new_states)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size)\n",
    "\n",
    "        q_target = deepcopy(q_eval)\n",
    "        q_target[batch_index, actions] = rewards + \\\n",
    "            self.gamma * np.max(q_next, axis=1) * dones\n",
    "\n",
    "        loss = self.policy_network.train_on_batch(states, q_target)\n",
    "\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, 0.1)\n",
    "\n",
    "        if(self.replay_mem.mem_centr % self.target_update_freq == 0):\n",
    "            self.target_network.set_weights(self.policy_network.get_weights())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def save_model(self, file_path='./model/tf_ddqn_model.model'):\n",
    "        self.policy_network.save(file_path)\n",
    "\n",
    "    def load_model(self, file_path='./model/tf_ddqn_model.model'):           \n",
    "        self.policy_network = load_model(file_path)\n",
    "        \n",
    "        self.target_network = build_dqn(lr=lr, n_actions=self.n_actions)\n",
    "        self.target_network.set_weights(self.policy_network.get_weights())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_decay = 1 - (2e-5)\n",
    "\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size = 1024\n",
    "batch_size = 32\n",
    "\n",
    "target_update_frequency = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(n_actions=env.action_space.n, input_dims=env.observation_space.shape,\n",
    "lr=lr, gamma=gamma, mem_size=mem_size, batch_size=batch_size,\n",
    "epsilon_decay=epsilon_decay, target_update_frequency=target_update_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "eps = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(range(episodes))\n",
    "\n",
    "for i in pbar:\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    ep_loss = []\n",
    "\n",
    "    while not(done):\n",
    "\n",
    "        action = agent.choose_action(obs)\n",
    "        \n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        #env.render()\n",
    "\n",
    "        score = score + reward\n",
    "        \n",
    "        agent.store_memory(state=obs, action=action, reward=reward, new_state=new_obs, done=done)\n",
    "\n",
    "        obs = deepcopy(new_obs)\n",
    "\n",
    "        loss = agent.train()\n",
    "        ep_loss.append(loss)\n",
    "    \n",
    "    scores.append(score)\n",
    "    eps.append(agent.epsilon)\n",
    "    losses.append(ep_loss)\n",
    "    pbar.set_description(\"Current_score = %s\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps, label=\"epsilon\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/tf/ddqn/epsilon.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_array = []\n",
    "for x in losses:\n",
    "    losses_array.append(np.mean(np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_array, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/tf/ddqn/losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_losses = np.array(pd.Series(np.array(losses_array)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_losses, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/tf/ddqn/losses_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/tf/ddqn/rewards.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_rewards = np.array(pd.Series(np.array(scores)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_rewards, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/tf/ddqn/rewards_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v2')\n",
    "\n",
    "test_agent = Agent(n_actions=test_env.action_space.n, input_dims=test_env.observation_space.shape)\n",
    "\n",
    "test_agent.epsilon = 0.0\n",
    "test_agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "\n",
    "pbar = tqdm(range(test_episodes))\n",
    "\n",
    "for i in pbar:\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = test_env.reset()\n",
    "    test_env.render()\n",
    "\n",
    "    while not(done):\n",
    "        action = test_agent.choose_action(obs)\n",
    "        \n",
    "        new_obs, reward, done, _ = test_env.step(action)\n",
    "        test_env.render()\n",
    "\n",
    "        score = score + reward\n",
    "\n",
    "        obs = deepcopy(new_obs)\n",
    "    \n",
    "    pbar.set_description(\"Current_score = %s\" % score)\n",
    "    print(\"score in episode \", (i+1) ,\" : \",score)\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
