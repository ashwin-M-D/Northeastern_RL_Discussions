{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c750b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, mem_size, batch_size, input_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_centr = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.state_memory = np.zeros(\n",
    "            (self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros(\n",
    "            (self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "\n",
    "    def store_transitions(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_centr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "\n",
    "        self.mem_centr = self.mem_centr + 1\n",
    "\n",
    "    def is_sampleable(self):\n",
    "        if self.mem_centr >= self.batch_size:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        if not(self.is_sampleable()):\n",
    "            return []\n",
    "        else:\n",
    "            batch = np.random.choice(\n",
    "                self.mem_size, self.batch_size, replace=False)\n",
    "\n",
    "            states = self.state_memory[batch]\n",
    "            new_states = self.new_state_memory[batch]\n",
    "            actions = self.action_memory[batch]\n",
    "            rewards = self.reward_memory[batch]\n",
    "            terminals = self.terminal_memory[batch]\n",
    "\n",
    "            return states, new_states, actions, rewards, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6926f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dims[0], 16)\n",
    "        self.layer2 = nn.Linear(16, 16)\n",
    "        self.layer3 = nn.Linear(16, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        l1 = self.layer1(x)\n",
    "        l1 = F.relu(l1)\n",
    "        l2 = self.layer2(l1)\n",
    "        l2 = F.relu(l2)\n",
    "        l3 = self.layer3(l2)\n",
    "        \n",
    "        output = l3\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Force Use a Device\n",
    "#device = 'cuda' #for GPU\n",
    "#device = 'cpu'  #for CPU\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276871d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, input_dims,\n",
    "                 lr=1e-4, gamma=0.9, mem_size=128, batch_size=64,\n",
    "                  epsilon_decay=0.995, target_update_frequency=256):\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_frequency\n",
    "\n",
    "        self.policy_network = NeuralNetwork(input_dims=input_dims, n_actions=n_actions).to(device)\n",
    "        self.target_network = deepcopy(self.policy_network)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr = lr)\n",
    "\n",
    "        self.replay_mem = ReplayBuffer(\n",
    "            mem_size=mem_size, batch_size=batch_size, input_dims=input_dims)\n",
    "\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            obs_T = torch.tensor(obs, device=device).float()\n",
    "            with torch.no_grad():\n",
    "                policy_values = self.policy_network(obs_T).cpu().detach().numpy()\n",
    "            action = np.argmax(policy_values)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_memory(self, state, action, reward, new_state, done):\n",
    "        self.replay_mem.store_transitions(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if not(self.replay_mem.is_sampleable()):\n",
    "            return 0\n",
    "\n",
    "        states, new_states, actions, rewards, dones = self.replay_mem.sample_buffer()\n",
    "        \n",
    "        states_T = torch.tensor(states, device=device).float()\n",
    "        new_states_T = torch.tensor(new_states, device=device).float()\n",
    "        rewards_T = torch.tensor(rewards, device=device).float()\n",
    "        actions_T = torch.tensor(actions, device=device).type(torch.int64).unsqueeze(1)\n",
    "        \n",
    "        q_eval = self.policy_network(states_T).gather(1, actions_T).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_network(new_states_T).max(1)[0].detach()\n",
    "        q_target = rewards_T + ( gamma * q_next )\n",
    "\n",
    "        loss = self.loss_function(q_eval, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "        if(self.replay_mem.mem_centr % self.target_update_freq == 0):\n",
    "            self.target_network = deepcopy(self.policy_network)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self, file_path='./model/torch_ddqn_model.model'):\n",
    "        torch.save(self.policy_network.state_dict(), file_path)\n",
    "\n",
    "    def load_model(self, file_path='./model/torch_ddqn_model.model'):\n",
    "        self.policy_network.load_state_dict(torch.load(file_path))\n",
    "        self.target_network = deepcopy(self.policy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973047f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_decay = 1 - (3e-5)\n",
    "\n",
    "episodes = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa48c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size = 1024\n",
    "batch_size = 128\n",
    "\n",
    "target_update_frequency = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(n_actions=env.action_space.n, input_dims=env.observation_space.shape,\n",
    "lr=lr, gamma=gamma, mem_size=mem_size, batch_size=batch_size,\n",
    "epsilon_decay=epsilon_decay, target_update_frequency=target_update_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b43ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "eps = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(episodes))\n",
    "\n",
    "for i in pbar:\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    ep_loss = []\n",
    "\n",
    "    while not(done):\n",
    "\n",
    "        action = agent.choose_action(obs)\n",
    "        \n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        #env.render()\n",
    "\n",
    "        score = score + reward\n",
    "        \n",
    "        agent.store_memory(state=obs, action=action, reward=reward, new_state=new_obs, done=done)\n",
    "\n",
    "        obs = deepcopy(new_obs)\n",
    "\n",
    "        loss = agent.train()\n",
    "        ep_loss.append(loss)\n",
    "    \n",
    "    scores.append(score)\n",
    "    eps.append(agent.epsilon)\n",
    "    losses.append(ep_loss)\n",
    "    pbar.set_description(\"Current_score = %s\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93492aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps, label=\"epsilon\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/torch/ddqn/epsilon.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_array = []\n",
    "for x in losses:\n",
    "    losses_array.append(np.mean(np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_array, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/torch/ddqn/losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_losses = np.array(pd.Series(np.array(losses_array)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_losses, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/torch/ddqn/losses_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/torch/ddqn/rewards.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_rewards = np.array(pd.Series(np.array(scores)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_rewards, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.savefig('./plots/torch/ddqn/rewards_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('LunarLander-v2')\n",
    "\n",
    "test_agent = Agent(n_actions=test_env.action_space.n, input_dims=test_env.observation_space.shape)\n",
    "\n",
    "test_agent.epsilon = 0.0\n",
    "test_agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "\n",
    "pbar = tqdm(range(test_episodes))\n",
    "\n",
    "for i in pbar:\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = test_env.reset()\n",
    "    test_env.render()\n",
    "\n",
    "    while not(done):\n",
    "        action = test_agent.choose_action(obs)\n",
    "        \n",
    "        new_obs, reward, done, _ = test_env.step(action)\n",
    "        test_env.render()\n",
    "\n",
    "        score = score + reward\n",
    "\n",
    "        obs = deepcopy(new_obs)\n",
    "    \n",
    "    scores.append(score)\n",
    "    pbar.set_description(\"Current_score = %s\" % score)\n",
    "    print(\"score in episode \", i ,\" : \",score)\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
