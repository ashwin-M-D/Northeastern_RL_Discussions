{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Double Deep Q Networks using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment to train our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class gridworld_custom(gym.Env):\n",
    "\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(gridworld_custom, self).__init__()\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.reward_range = (-10, 100)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=np.array(\n",
    "            [0, 0]), high=np.array([4, 4]), dtype=np.int64)\n",
    "\n",
    "        self.target_coord = (4, 4)\n",
    "        self.death_coord = [(3, 1), (4, 2)]\n",
    "\n",
    "    def Reward_Function(self, obs):\n",
    "\n",
    "        if (obs[0] == self.target_coord[0] and obs[1] == self.target_coord[1]):\n",
    "            return 20\n",
    "        elif (obs[0] == self.death_coord[0][0] and obs[1] == self.death_coord[0][1]) or \\\n",
    "                (obs[0] == self.death_coord[1][0] and obs[1] == self.death_coord[1][1]):\n",
    "            return -10\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.prev_obs = [random.randint(0, 4), random.randint(0, 4)]\n",
    "\n",
    "        if (self.prev_obs[0] == self.target_coord[0] and self.prev_obs[1] == self.target_coord[1]):\n",
    "\n",
    "            return self.reset()\n",
    "\n",
    "        return self.prev_obs\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        action = int(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = deepcopy(self.prev_obs)\n",
    "\n",
    "        if(action == 0):\n",
    "            if(self.prev_obs[0] < 4):\n",
    "                obs[0] = obs[0] + 1\n",
    "            else:\n",
    "                obs[0] = obs[0]\n",
    "\n",
    "        if(action == 1):\n",
    "            if(self.prev_obs[0] > 0):\n",
    "                obs[0] = obs[0] - 1\n",
    "            else:\n",
    "                obs[0] = obs[0]\n",
    "\n",
    "        if(action == 2):\n",
    "            if(self.prev_obs[1] < 4):\n",
    "                obs[1] = obs[1] + 1\n",
    "            else:\n",
    "                obs[1] = obs[1]\n",
    "\n",
    "        if(action == 3):\n",
    "            if(self.prev_obs[1] > 0):\n",
    "                obs[1] = obs[1] - 1\n",
    "            else:\n",
    "                obs[1] = obs[1]\n",
    "\n",
    "        reward = self.Reward_Function(obs)\n",
    "\n",
    "        if (obs[0] == self.target_coord[0] and obs[1] == self.target_coord[1]) or (self.current_step >= 250):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.prev_obs = obs\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "\n",
    "        for i in range(0, 5):\n",
    "            for j in range(0, 5):\n",
    "                if i == self.prev_obs[0] and j == self.prev_obs[1]:\n",
    "                    print(\"*\", end=\" \")\n",
    "                elif i == self.target_coord[0] and j == self.target_coord[1]:\n",
    "                    print(\"w\", end=\" \")\n",
    "                elif (i == self.death_coord[0][0] and j == self.death_coord[0][1]) or \\\n",
    "                     (i == self.death_coord[1][0] and j == self.death_coord[1][1]):\n",
    "                    print(\"D\", end=\" \")\n",
    "                else:\n",
    "                    print(\"_\", end=\" \")\n",
    "            print()\n",
    "\n",
    "        print()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from statistics import mean\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(2, 8)\n",
    "        self.layer2 = nn.Linear(8, 8)\n",
    "        self.layer3 = nn.Linear(8, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        l1 = self.layer1(x)\n",
    "        l1 = F.relu(l1)\n",
    "        l2 = self.layer2(l1)\n",
    "        l2 = F.relu(l2)\n",
    "        l3 = self.layer3(l2)\n",
    "        \n",
    "        output = l3\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if there is a GPU which can be used to accelerate the workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Force Use a Device\n",
    "#device = 'cuda' #for GPU\n",
    "#device = 'cpu'  #for CPU\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the neuralnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = NeuralNetwork().to(device)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gridworld_custom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check up the functionality of epsilon greedy. Just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "episodes = 5000\n",
    "epsilon_copy = deepcopy(epsilon)\n",
    "eps = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    epsilon_copy = epsilon_copy * epsilon_decay\n",
    "    eps.append(epsilon_copy)\n",
    "\n",
    "plt.plot(eps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "pbar = tqdm(range(episodes))\n",
    "\n",
    "last_loss = 0.0\n",
    "\n",
    "losses_array = []\n",
    "rewards_array = []\n",
    "\n",
    "for episode in pbar:\n",
    "    \n",
    "    prev_obs = env.reset()\n",
    "    done = False\n",
    "    mem_size = 0\n",
    "    \n",
    "    curr_state_mem = np.array([[0,0]] * batch_size)\n",
    "    prev_state_mem = np.array([[0,0]] * batch_size)\n",
    "    action_mem = np.array([0] * batch_size)\n",
    "    reward_mem = np.array([0] * batch_size)\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    epsilon = epsilon * epsilon_decay\n",
    "\n",
    "    while not(done) :\n",
    "        \n",
    "        if(random.uniform(0, 1) > epsilon):\n",
    "            with torch.no_grad():\n",
    "                prev_q = q_network(torch.tensor(prev_obs, device=device).float())\n",
    "                prev_q = prev_q.cpu().detach().numpy()\n",
    "            action = np.argmax(prev_q)\n",
    "        else:\n",
    "            action = random.randint(0,3)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        prev_state_mem[mem_size] = prev_obs\n",
    "        curr_state_mem[mem_size] = obs\n",
    "        action_mem[mem_size] = action\n",
    "        reward_mem[mem_size] = reward\n",
    "        mem_size = mem_size + 1\n",
    "\n",
    "        prev_obs = obs\n",
    "\n",
    "        if(mem_size == batch_size):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                target_q = q_network(torch.tensor(curr_state_mem, device=device).float()).max(1)[0].detach()\n",
    "\n",
    "                expected_q_mem = torch.tensor(reward_mem, device=device).float() + ( gamma * target_q )\n",
    "\n",
    "            network_q_mem = q_network(torch.tensor(prev_state_mem, device=device).float()).gather(1, torch.tensor(action_mem, device=device).type(torch.int64).unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            loss = loss_function(network_q_mem, expected_q_mem)\n",
    "\n",
    "            last_loss = \"{:.3f}\".format(loss.item())\n",
    "            \n",
    "            mem_size = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    pbar.set_description(\"loss = %s\" % last_loss)\n",
    "    losses_array.append(last_loss)\n",
    "    rewards_array.append(mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_array, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_losses = np.array(pd.Series(np.array(losses_array)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_losses, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_array, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot reward trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 50\n",
    "\n",
    "cumsum_rewards = np.array(pd.Series(np.array(rewards_array)).rolling(window=resolution).mean() )\n",
    "\n",
    "plt.plot(cumsum_rewards, label=\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while not(done):\n",
    "    with torch.no_grad():\n",
    "        prev_q = q_network(torch.tensor(prev_obs, device=device).float())\n",
    "        prev_q = prev_q.cpu().detach().numpy()\n",
    "    action = np.argmax(prev_q)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    prev_obs = obs\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26dc2a88388ca3fcbf0140d7c68165201b68af8ed361ccf7d4f5e19cb8c25bc6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
