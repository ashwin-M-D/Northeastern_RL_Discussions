{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment to train our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class gridworld_custom(gym.Env):\n",
    "\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(gridworld_custom, self).__init__()\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.reward_range = (-5, 5)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=np.array(\n",
    "            [0, 0]), high=np.array([4, 4]), dtype=np.int64)\n",
    "\n",
    "        self.target_coord = (4, 4)\n",
    "        self.death_coord = [(3, 1), (4, 2)]\n",
    "\n",
    "    def Reward_Function(self, obs):\n",
    "\n",
    "        if (obs[0] == self.target_coord[0] and obs[1] == self.target_coord[1]):\n",
    "            return 5\n",
    "        elif (obs[0] == self.death_coord[0][0] and obs[1] == self.death_coord[0][1]) or \\\n",
    "                (obs[0] == self.death_coord[1][0] and obs[1] == self.death_coord[1][1]):\n",
    "            return -5\n",
    "        else:\n",
    "            return -0.1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.prev_obs = [random.randint(0, 4), random.randint(0, 4)]\n",
    "\n",
    "        if (self.prev_obs[0] == self.target_coord[0] and self.prev_obs[1] == self.target_coord[1]) or \\\n",
    "            (self.prev_obs[0] == self.death_coord[0][0] and self.prev_obs[1] == self.death_coord[0][1]) or \\\n",
    "                (self.prev_obs[0] == self.death_coord[1][0] and self.prev_obs[1] == self.death_coord[1][1]):\n",
    "\n",
    "            return self.reset()\n",
    "\n",
    "        return self.prev_obs\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        action = int(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self.prev_obs\n",
    "\n",
    "        if(action == 0):\n",
    "            if(self.prev_obs[0] < 4):\n",
    "                obs[0] = self.prev_obs[0] + 1\n",
    "            else:\n",
    "                obs[0] = self.prev_obs[0]\n",
    "\n",
    "        if(action == 1):\n",
    "            if(self.prev_obs[0] > 0):\n",
    "                obs[0] = self.prev_obs[0] - 1\n",
    "            else:\n",
    "                obs[0] = self.prev_obs[0]\n",
    "\n",
    "        if(action == 2):\n",
    "            if(self.prev_obs[1] < 4):\n",
    "                obs[1] = self.prev_obs[1] + 1\n",
    "            else:\n",
    "                obs[1] = self.prev_obs[1]\n",
    "\n",
    "        if(action == 3):\n",
    "            if(self.prev_obs[1] > 0):\n",
    "                obs[1] = self.prev_obs[1] - 1\n",
    "            else:\n",
    "                obs[1] = self.prev_obs[1]\n",
    "\n",
    "        reward = self.Reward_Function(obs)\n",
    "\n",
    "        if (obs[0] == self.target_coord[0] and obs[1] == self.target_coord[1]) or (self.current_step >= 250):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.prev_obs = obs\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "\n",
    "        for i in range(0, 5):\n",
    "            for j in range(0, 5):\n",
    "                if i == self.prev_obs[0] and j == self.prev_obs[1]:\n",
    "                    print(\"*\", end=\" \")\n",
    "                elif i == self.target_coord[0] and j == self.target_coord[1]:\n",
    "                    print(\"w\", end=\" \")\n",
    "                elif (i == self.death_coord[0][0] and j == self.death_coord[0][1]) or \\\n",
    "                     (i == self.death_coord[1][0] and j == self.death_coord[1][1]):\n",
    "                    print(\"D\", end=\" \")\n",
    "                else:\n",
    "                    print(\"_\", end=\" \")\n",
    "            print()\n",
    "\n",
    "        print()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the q-learning class which contains the table storing all the q values for all states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_learning():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.q_table = pd.DataFrame(columns=['state', 'q_val_0', 'q_val_1', 'q_val_2', 'q_val_3'])\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                state_str = \"(\"+str(i)+\",\"+str(j)+\")\"\n",
    "                X = pd.DataFrame([[state_str, 0, 0, 0, 0]], columns=['state', 'q_val_0', 'q_val_1', 'q_val_2', 'q_val_3'])\n",
    "                self.q_table = self.q_table.append(X, ignore_index=True)\n",
    "        \n",
    "        self.q_table = self.q_table.set_index('state')\n",
    "\n",
    "        self.gamma = 1\n",
    "        self.step_size = 0.8\n",
    "    \n",
    "    def update_q_value(self, curr_state, prev_state, action, reward):\n",
    "\n",
    "        curr_state_str = \"(\"+str(curr_state[0])+\",\"+str(curr_state[1])+\")\"\n",
    "        prev_state_str = \"(\"+str(prev_state[0])+\",\"+str(prev_state[1])+\")\"\n",
    "        action_str = \"q_val_\"+str(action)\n",
    "\n",
    "        q_pred = self.q_table.loc[prev_state_str][action_str]\n",
    "\n",
    "        q_target = reward + self.gamma * np.max(self.q_table.loc[curr_state_str].to_numpy())\n",
    "\n",
    "        self.q_table.loc[prev_state_str][action_str] = q_pred + self.step_size * (q_target - q_pred)\n",
    "\n",
    "    def choose_action(self, curr_state):\n",
    "        curr_state_str = \"(\"+str(curr_state[0])+\",\"+str(curr_state[1])+\")\"\n",
    "        action = np.argmax(self.q_table.loc[curr_state_str].to_numpy())\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check up the functionality of epsilon greedy. Just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_decay = 0.9995\n",
    "eps = []\n",
    "for i in range(5000):\n",
    "    epsilon = epsilon * epsilon_decay\n",
    "    eps.append(epsilon)\n",
    "\n",
    "plt.plot(eps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gridworld_custom()\n",
    "agent = q_learning()\n",
    "\n",
    "episodes = 5000\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "pbar = tqdm(range(episodes))\n",
    "\n",
    "for episode in pbar:\n",
    "\n",
    "    prev_obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    epsilon = epsilon * epsilon_decay\n",
    "\n",
    "    while not(done):\n",
    "        if(random.uniform(0, 1) > epsilon):\n",
    "            action = agent.choose_action(prev_obs)\n",
    "        else:\n",
    "            action = random.randint(0,3)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.update_q_value(obs, prev_obs, action, reward)\n",
    "        prev_obs = obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the Q Table after training. Gives us an understanding as to how the model might function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while not(done):\n",
    "    action = agent.choose_action(prev_obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    prev_obs = obs\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dab403dcfa4a64bee3ff417c650bc5376500f360e3ead239cab01b685475af7b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
