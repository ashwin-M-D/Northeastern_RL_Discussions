{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lunarlander_pytorch_DDQN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lunar Lander Environment with pytorch\n",
        "\n",
        " Information regarding environment: https://gym.openai.com/envs/LunarLander-v2/\n",
        "\n",
        "\n",
        "Variables associated with the state space: \n",
        "- x coordinate of the lander\n",
        "- y coordinate of the lander\n",
        "- vx, the horizontal velocity\n",
        "- vy, the vertical velocity\n",
        "- θ, the orientation in space\n",
        "- vθ, the angular velocity\n",
        "- Left leg touching the ground (Boolean)\n",
        "- Right leg touching the ground (Boolean)\n"
      ],
      "metadata": {
        "id": "I26TL_diwynO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "_-EaBWDJyi4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installations for rendering\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "X_dnkVwFRxNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB8vPA3rwyLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b45e3e9-e2ff-45b3-eae0-ac0e3bc0115a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f2a71296750>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "\n",
        "# Rendering\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "sKWl1Ls1yJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build neural network"
      ],
      "metadata": {
        "id": "q35BFh6Uynle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_dims, n_actions):\n",
        "    \"\"\"\n",
        "    input_dims = input shape (number of features)\n",
        "    n_actions = number of neurons in output layer\n",
        "    \"\"\"\n",
        "    super(NeuralNet, self).__init__() # Importing all attributes from parent\n",
        "\n",
        "    self.layer1 = nn.Linear(input_dims, 32) # Simple linear network\n",
        "    self.layer2 = nn.Linear(32, 16)\n",
        "    self.layer3 = nn.Linear(16, n_actions)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    To pass the results forward in the neural net\n",
        "    Args:\n",
        "      x: input data\n",
        "    \"\"\"\n",
        "    l1 = self.layer1(x)\n",
        "    l1_ =self.activation(l1)\n",
        "    l2 = self.layer2(l1_)\n",
        "    l2_ = self.activation(l2)\n",
        "    l3 = self.layer3(l2_)\n",
        "    return l3\n"
      ],
      "metadata": {
        "id": "v-qGvuBxyKc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay memory"
      ],
      "metadata": {
        "id": "5blIg1Iv8ode"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, mem_size, batch_size,input_dims):\n",
        "    \"\"\"\n",
        "    Initialization of replay buffer, creating different numpy arrays for each component\n",
        "    Input dims can be n-dimensional value\n",
        "    \"\"\"\n",
        "    self.mem_size = mem_size\n",
        "    self.batch_size = batch_size\n",
        "    self.mem_counter = 0 # memory counter\n",
        "    self.state_mem = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "    self.action_mem = np.zeros(self.mem_size, dtype=np.float32)\n",
        "    self.reward_mem = np.zeros(self.mem_size, dtype=np.float32)\n",
        "    self.next_state_mem = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "    self.terminal_mem = np.zeros(self.mem_size, dtype=np.float32) # done represents zero\n",
        "\n",
        "  def store_record(self, state, action, reward, state_, done):\n",
        "    \"\"\"\n",
        "    Stores the current values on the memory\n",
        "    \"\"\"\n",
        "    index = self.mem_counter % self.mem_size # once is full, it restores it\n",
        "    self.state_mem[index] = state\n",
        "    self.action_mem[index] = action\n",
        "    self.reward_mem[index] = reward\n",
        "    self.next_state_mem[index] = state_ # can also be named \"next_state\"\n",
        "    self.terminal_mem[index] = 1 - int(done) # change boolean to 1-0\n",
        "    self.mem_counter = self.mem_counter + 1\n",
        "  def is_sampleable(self):\n",
        "    \"\"\"\n",
        "    Tells if can be sampled\n",
        "    \"\"\"\n",
        "    if(self.mem_counter >= self.batch_size):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "  def sample_buffer(self):\n",
        "    \"\"\"\n",
        "    Samples from buffer, do not sample any zeros\n",
        "    Output tuple with the sample\n",
        "    \"\"\"\n",
        "    if not(self.is_sampleable()):\n",
        "      return []\n",
        "\n",
        "    max_mem = min(self.mem_size, self.mem_counter) # make sure it does not sample zeros\n",
        "\n",
        "    \n",
        "    batch = np.random.choice(max_mem,\n",
        "                             self.batch_size, \n",
        "                             replace=False) # sampling, with no replace\n",
        "\n",
        "    states = self.state_mem[batch]\n",
        "    actions = self.action_mem[batch]\n",
        "    rewards = self.reward_mem[batch]\n",
        "    next_states = self.next_state_mem[batch]\n",
        "    terminals = self.terminal_mem[batch]\n",
        "\n",
        "    return states, actions, rewards, next_states, terminals\n",
        "\n"
      ],
      "metadata": {
        "id": "HJEIvrYz0zqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent - DQN"
      ],
      "metadata": {
        "id": "gCZww6IL8q5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "inNeI4EmROyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dqn_agent():\n",
        "  def __init__(self, \n",
        "               input_dims, \n",
        "               n_actions,\n",
        "               epsilon_decay=(1-(1e-4)), # Number close to 1\n",
        "               gamma=0.99, \n",
        "               lr=1e-4, \n",
        "               mem_size=1024, \n",
        "               batch_size=64):\n",
        "    \"\"\"\n",
        "    DQN Agent\n",
        "    Args: \n",
        "      epsilon_decay: factor which is going to decay toward zero\n",
        "      gamma : discounted factor in Bellman equation \n",
        "      lr : learning rate\n",
        "      mem_size : memory size \n",
        "      batch_size : batch size, should be a power of 2\n",
        "    \"\"\"\n",
        "    self.input_dims = input_dims[0]\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon = 1\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.lr = lr\n",
        "\n",
        "    self.mem_size = mem_size\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.replay_mem = ReplayBuffer(mem_size = mem_size, \n",
        "                                   batch_size = batch_size, \n",
        "                                   input_dims = input_dims)\n",
        "    \n",
        "    self.policy_network = NeuralNet( input_dims = self.input_dims,\n",
        "                                  n_actions = n_actions).to(device)\n",
        "    self.loss_function = nn.MSELoss()\n",
        "    self.optimizer = torch.optim.Adam(self.policy_network.parameters(),\n",
        "                                      lr = lr)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "    \n",
        "  def choose_action(self, obs):\n",
        "    \"\"\"\n",
        "    Agent to choose an action at any state\n",
        "    Arg: \n",
        "      obs: Is the state (features) \n",
        "\n",
        "    Returns: \n",
        "      action to take\n",
        "    \"\"\"\n",
        "    if(np.random.random() > self.epsilon):\n",
        "      obs_T = torch.tensor(obs, device = device).float()\n",
        "      with torch.no_grad(): # no_grad is to calculate the action first, means no gradient\n",
        "        q_values = self.policy_network(obs_T).cpu().detach().numpy() # only getting the q values \n",
        "        action = np.argmax(q_values)\n",
        "        # No replay memory because that is for training\n",
        "    else: # we want it to explore\n",
        "      action = np.random.randint(self.n_actions)\n",
        "    \n",
        "    return action\n",
        "\n",
        "  def store_mem(self, state, action, reward, state_, done):\n",
        "    \"\"\"\n",
        "    Stores in the memory \n",
        "    \"\"\"\n",
        "    self.replay_mem.store_record(state, action, reward, state_, done)\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"\n",
        "    Training with replay memory \n",
        "    \"\"\"\n",
        "    if not(self.replay_mem.is_sampleable()):\n",
        "      return np.nan # if not sampleable, don't train and return nan loss\n",
        "    \n",
        "    states, actions, rewards, next_states, dones = self.replay_mem.sample_buffer()\n",
        "\n",
        "    states_T = torch.tensor(states, device = device).float()\n",
        "    actions_T = torch.tensor(actions, device = device).type(torch.int64).unsqueeze(1) # unsqueeze converts into higher dimension, this we only use it as a key\n",
        "    rewards_T = torch.tensor(rewards, device = device).float()\n",
        "    next_states_T = torch.tensor(next_states, device = device).float()\n",
        "    dones_T = torch.tensor(dones, device = device).type(torch.int64)\n",
        "\n",
        "    q_values = self.policy_network(states_T).gather(1, actions_T).squeeze(1) # gathering q values from the network for that action\n",
        "\n",
        "    with torch.no_grad():\n",
        "      q_values_next = self.policy_network(next_states_T).max(1)[0].detach() # max q value\n",
        "\n",
        "    q_target_values = rewards_T + self.gamma * q_values_next * dones\n",
        "\n",
        "    loss = self.loss_function(q_values, q_target_values)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.epsilon = self.epsilon * self.epsilon_decay\n",
        "\n",
        "    return loss.item() # loss.item is just the value\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W2mG0TNy8thV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters"
      ],
      "metadata": {
        "id": "n2z5vNaBAaTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lr = 1e-4\n",
        "gamma = 0.99\n",
        "epsilon_decay = 1- (3e-5)\n",
        "\n",
        "episodes = 700\n",
        "\n",
        "mem_size = 10000\n",
        "batch_size = 64\n"
      ],
      "metadata": {
        "id": "TFdPyfmaQ9fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "OfA92m2K8ttG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTgNKiteRkXI",
        "outputId": "85f39e01-8f1d-4700-c697-24ace543f792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h6T01opy8Kzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "mGNCt3AM8La-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = dqn_agent(input_dims=env.observation_space.shape, \n",
        "                  n_actions=env.action_space.n, \n",
        "                  epsilon_decay=epsilon_decay, \n",
        "                  gamma = gamma,\n",
        "                  lr = lr,\n",
        "                  mem_size = mem_size, \n",
        "                  batch_size = batch_size\n",
        "                  )"
      ],
      "metadata": {
        "id": "3eElUIv2SQqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "eps = []\n",
        "losses = []"
      ],
      "metadata": {
        "id": "8zVBWeg4Tfkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = tqdm(range(episodes))\n",
        "\n",
        "for i in pbar: \n",
        "  done = False\n",
        "  score = 0\n",
        "  state = env.reset()\n",
        "  ep_loss = []\n",
        "  prev_screen = env.render(mode='rgb_array')\n",
        "  plt.imshow(prev_screen)\n",
        "\n",
        "  while not(done):\n",
        "    # Agent chooses action\n",
        "    action = agent.choose_action(state)\n",
        "    \n",
        "    # Environment's new state, reward and others\n",
        "    new_state, reward, done, _ = env.step(action) #\"_\" is info to debug\n",
        "\n",
        "    # Rendering\n",
        "    screen = env.render(mode='rgb_array')\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "\n",
        "    score = score + reward\n",
        "\n",
        "    # Store record in memory\n",
        "    agent.store_mem(state, action, reward, new_state, done)\n",
        "\n",
        "    # Copy of state\n",
        "    state = deepcopy(new_state)\n",
        "\n",
        "    # Keep track of loss\n",
        "    loss = agent.train()\n",
        "    ep_loss.append(loss) # episode loss\n",
        "  \n",
        "  # At the end of episode, do the following: \n",
        "  scores.append(score)\n",
        "  eps.append(agent.epsilon)\n",
        "  losses.append(mean(ep_loss))\n",
        "  pbar.set_description(\"current score = %s\" % score)\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "QIap7pPsTkz9",
        "outputId": "88f07a7a-afda-4289-8801-3c5f922420e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent - DDQN"
      ],
      "metadata": {
        "id": "3V_aKrTP8y09"
      }
    }
  ]
}