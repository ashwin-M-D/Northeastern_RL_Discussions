{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b32f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017aff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dims, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, n_actions)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l1 = self.layer1(x)\n",
    "        l1_ = self.activation(l1)\n",
    "        l2 = self.layer2(l1_)\n",
    "        l2_ = self.activation(l2)\n",
    "        l3 = self.layer3(l2_)\n",
    "        \n",
    "        return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3044e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, mem_size, batch_size, input_dims):\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.state_mem = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_mem = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_mem = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.terminal_mem = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        \n",
    "    def store_record(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        \n",
    "        self.state_mem[index] = state\n",
    "        self.action_mem[index] = action\n",
    "        self.reward_mem[index] = reward\n",
    "        self.next_state_mem[index] = state_\n",
    "        self.terminal_mem[index] = 1 - int(done)\n",
    "        self.mem_counter = self.mem_counter + 1\n",
    "    \n",
    "    def is_sampleable(self):\n",
    "        if(self.mem_counter >= self.batch_size):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def sample_buffer(self):\n",
    "        \n",
    "        if not(self.is_sampleable()):\n",
    "            return []\n",
    "        \n",
    "        max_mem = min(self.mem_size, self.mem_counter)\n",
    "        \n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        states = self.state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        terminals = self.terminal_mem[batch]\n",
    "        \n",
    "        return states, actions, rewards, next_states, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5399ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn_agent():\n",
    "    \n",
    "    def __init__(self, input_dims, n_actions, epsilon_decay=(1 - (1e-4)), gamma=0.99, lr=1e-4,\\\n",
    "                 mem_size=1024, batch_size=128):\n",
    "        \n",
    "        self.input_dims = input_dims[0]\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon = 1\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.replay_mem = ReplayBuffer(mem_size=mem_size, batch_size=batch_size, input_dims=input_dims)\n",
    "        self.policy_network = NeuralNet(input_dims=self.input_dims, n_actions=n_actions).to(device)\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if(np.random.random() > self.epsilon):\n",
    "            with torch.no_grad():\n",
    "                state_T = torch.tensor(state, device=device).float()\n",
    "                q_values = self.policy_network(state_T).cpu().detach().numpy()\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "\n",
    "        return action\n",
    "            \n",
    "    def store_mem(self, state, action, reward, state_, done):\n",
    "        self.replay_mem.store_record(state, action, reward, state_, done)\n",
    "\n",
    "    def train(self):\n",
    "        if not(self.replay_mem.is_sampleable()):\n",
    "            return np.nan\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_mem.sample_buffer()\n",
    "\n",
    "        states_T = torch.tensor(states, device=device).float()\n",
    "        actions_T = torch.tensor(actions, device=device).type(torch.int64).unsqueeze(1)\n",
    "        rewards_T = torch.tensor(rewards, device=device).float()\n",
    "        next_states_T = torch.tensor(next_states, device=device).float()\n",
    "        dones_T = torch.tensor(dones, device=device).type(torch.int64)\n",
    "\n",
    "        q_values = self.policy_network(states_T).gather(1, actions_T).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values_next = self.policy_network(next_states_T).max(1)[0].detach()\n",
    "\n",
    "        q_target_values = rewards_T + self.gamma * q_values_next * dones_T\n",
    "\n",
    "        loss = self.loss_function(q_values, q_target_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505017cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_decay = 1 - (3e-5)\n",
    "episodes = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_size = 100000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b872bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = dqn_agent(input_dims=env.observation_space.shape, n_actions=env.action_space.n, epsilon_decay=epsilon_decay,\n",
    "gamma=gamma, lr=lr, mem_size=mem_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "eps = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(episodes))\n",
    "\n",
    "for i in pbar:\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    ep_loss = []\n",
    "\n",
    "    while not(done):\n",
    "        action = agent1.choose_action(state)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        score = score + reward\n",
    "\n",
    "        agent1.store_mem(state, action, reward, new_state, done)\n",
    "\n",
    "        state = deepcopy(new_state)\n",
    "\n",
    "        loss = agent1.train()\n",
    "        ep_loss.append(loss)\n",
    "    \n",
    "    scores.append(score)\n",
    "    eps.append(agent1.epsilon)\n",
    "    losses.append(mean(ep_loss))\n",
    "    pbar.set_description(\"current score = %s\" % score)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
